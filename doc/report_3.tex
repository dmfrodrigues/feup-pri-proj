\documentclass[sigconf, authorversion]{acmart}

\usepackage{svg}
\usepackage{hyperref}
\usepackage[]{siunitx}
\PassOptionsToPackage{obeyspaces}{url}
\usepackage{dblfloatfix}
\usepackage{amsmath}

\usepackage{listings}

\lstset{
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    frame=tb, % draw frame at top and bottom of the code
    tabsize=2, % tab space width
    numbers=left, % display line numbers on the left
    showstringspaces=true, % don't show spaces in strings
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\urlstyle{tt}

\newcommand{\matr}[1]{\textbf{#1}}

\settopmatter{printacmref=false}

\graphicspath{{../img/}}

\usepackage{currfile-abspath}

\sisetup{
    group-separator = {\,},
    range-units=single,
    range-phrase=--,
    group-minimum-digits=3
}

\getmainfile % get real main file (can be different than jobname in some cases)
\getabspath{\themainfile} % or use \jobname.tex instead (not as safe)
\let\mainabsdir\theabsdir % save result away (macro will be overwritten by the next \getabspath
\let\mainabspath\theabspath % save result away (macro will be overwritten by the next \getabspath

\begin{document}

\title{Information Retrieval System for European Union Legislation}

\author{Diogo Miguel F. Rodrigues}
\email{up201806429@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\author{João António C. V. B. Sousa}
\email{up201806613@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\author{Rafael Soares Ribeiro}
\email{up201806330@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\renewcommand{\shortauthors}{Rodrigues, Sousa and Ribeiro}

\begin{abstract}
    EUR-Lex is an European legislation database that offers access to European Union (EU) law,
    case-law by the Court of Justice of the EU and other public EU documents.
    This project aims to retrieve, process and prepare the data from this database, in order to create an information retrieval system of EU legislation.
    Various methods were used to achieve this, from the data collection and filtering phase, to the exploration of the useful data and finally its application in a search system.
    We have obtained a large dataset with \SI{99903}{} documents, which we indexed using Solr. We tested the existing system by running queries based on expected information needs, and evaluated the results' relevance: after manual judgement and subsequent automatic metric calculation, among the 7 queries, 6 yielded reasonable or very good results, proving the effectiveness of the search system.
    The system was improved through development of a front-end interface, using different query parsers, faceting, synonyms and page-rating algorithm, yielding similar results due to some improvements being counteracted by modifications that impacted performance negatively, such as improper usage of synonyms due to platform limitations.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317</concept_id>
<concept_desc>Information systems~Information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003381.10003382</concept_id>
<concept_desc>Information systems~Structured text search</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}
\ccsdesc[500]{Information systems~Structured text search}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, information processing, information retrieval, full-text search}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\maketitle

\section{Introduction}
\label{intro}
The European Union (EU) is an economic and political union of 27 member states of Europe. Its predecessors were the European Community for Coal and Steel (ECCS) and the European Economic Community (ECC), created in 1951 and 1958 in the wake of the Second World War. The EU maintains the fundamental goals the ECCS and ECC had of promoting peace and economic cooperation among countries so that a new war among European nations would be economically infeasible. The EU has over time expanded its objectives so as to protect what has been known as the \textit{European Values}, encoded into the Charter of Fundamental Rights \cite{cfr} and the founding treaties of the EU \cite{teu, tfeu}.
The Commission has the initiative for legislation, citizens are entitled to participate in the shaping of the EU legislation through the election of the European Parliament, and their respective state governments through the European Council, being the European Court of Justice responsible for enforcing EU law.

EU law is a system of rules operating within the EU member states, and covers topics from Constitutional and Administrative Law to Freedom of Movement, Trade and Public Regulation, Natural Resource Management and Social Market Regulations.
The three main European legislation databases are EUR-Lex, PreLex and OEIL. EUR-Lex contains information of all the documents of the Official Journal (OJ) of the EU starting at the year of 1951, PreLex has complete records starting at 1974 and is an extension of EUR-Lex that has many of the same documents, but focuses on inter-institutional procedures and data concerning different stages in the decision-making process and law that has not been settled yet, meaning it is most likely to contain drafts of EU legislation. Lastly, OEIL is the database of the European Parliament and stores inter-institutional decision-making in the EU but from the point of view of the Parliament.

Resorting to Solr, we developed a search system by exploring specific information needs. The retrieval tasks were translated into queries, followed by manual analysis of the results' relevance and by a performance comparison of metrics (precision and recall) across them, which is followed by some feature ideas to improve the system.

Finally, we present the implemented improvements and analyze their impact of the system, comparing the "before" and "after".

% slight changes in Introduction last 2 paragraphs

\section{Aspects of European legislation}

\begin{figure*}[b]
  \includegraphics[width=\textwidth]{diagram-pipeline.drawio}
  \caption{Data processing pipeline.}
\end{figure*}

\subsection{Identifier systems}

Most documents in EUR-Lex are assigned a unique, language independent CELEX (\textit{Communitatis Europae Lex}) number \cite{celex}. It has the format: Sector number -- Year (4 digits) -- Doc. type -- Doc. number.
ECLI (European Case-Law Identifier) is a uniform identifier used primarily in judicial decisions \cite{ecli}.
ELI (European Legislation Identifier) is a URI that can be read and used by humans and computers to refer to legislation. It is mostly associated with the OJ, and is endorsed by the European Council \cite{eli}.

\subsection{Classification systems}

The institutions of the EU use three classification systems under which they classify documents: the Directory of European Union Legislation, EuroVoc and subject matter.

The Directory of European Union Legislation (or otherwise \textit{directory codes}) is a numeric classification system used primarily by EUR-Lex. It assigns each area/sub-area of interest a number, and each area may contain many sub-areas; a directory code is a dot-separated list of area numbers. For example, a document related to agricultural research has directory code \texttt{03.30.50}: \textit{Agriculture} (\texttt{03}) $\rightarrow$ \textit{Agricultural structures} (\texttt{03.30}) $\rightarrow$ \textit{Agricultural research} (\texttt{03.30.50}) \cite{directory-legal-acts}.

The EuroVoc is a multilingual, multidisciplinary thesaurus covering the activities of the EU. It contains terms in 23 EU languages and 3 languages of candidate EU members, and is used by European institutions, the Publications Office of the EU, national and regional parliaments in Europe, and governments and private users around the world \cite{eurovoc}.

The subject matter authority table (AT) is a controlled vocabulary containing the concepts used for the indexation of notices published on EUR-Lex. It differs from directory codes and the EuroVoc in that the AT is strictly aligned with the evolution of EU policies cited in the different treaties of the EU \cite{subject-matter-at}.

\section{Dataset preparation}

\subsection{Metadata collection} \label{ssec:metadata-collection}

The dataset selected for this project was the EUR-Lex database. By making use of the \url{api.epdb.eu} API (namely of the \url{api.epdb.eu/eurlex/document} endpoint), we extracted a list of \SI{138911}{} document metadata entries, which we stored in file \texttt{raw.json}, with a size of \SI{387.8}{\mega\byte}.

For each document, the following fields were obtained:

\begin{itemize}
    \item \texttt{form}: Form (e.g., Agreement, Recommendation, ...)
    \item \texttt{date\_document}: Document date; generally date of signature.
    \item \texttt{title}: Document title.
    \item \texttt{oj\_date}: Date of publication in the OJ.
    \item \texttt{of\_effect}: Date of effect.
    \item \texttt{end\_validity}: Date the document validity ends.
    \item \texttt{addressee}: Addressee of the document (e.g., countries to which the document concerns).
    \item \texttt{subject\_matter}: Subject matters the document concerns.
    \item \texttt{directory\_codes}: List of directory codes.
    \item \texttt{eurovoc\_descriptors}: List of EuroVoc terms.
    \item \texttt{legal\_basis}: List of EU legislation identifiers (mostly CELEX IDs) that forms the legal basis of the current document.
    \item \texttt{relationships}: List of relationships between this document and other EUR-Lex documents (amendments, legal basis, ...).
    
    \item \texttt{eurlex\_perma\_url}: Permanent link of the document in the EUR-Lex website.

    \item \texttt{text\_url}: URL where document content is available.

    \item \texttt{doc\_id}: Document ID in the API.
    \item \texttt{api\_url}: URL of the document in the API
    \item \texttt{prelex\_relation}: List of documents in the PreLex database related to the current document.
    \item \texttt{internal\_ref}: Internal reference number of the document in the responsible body's separate database.
    \item \texttt{additional\_info}: Mostly used when the document has complex rules for start/end of validity.
\end{itemize}

\subsection{Data cleaning}
\label{ssec:data-cleaning}

After obtaining the metadata, we parsed \texttt{eurlex\_perma\_url} to obtain the CELEX identifiers. We decided to use CELEX as the primary document identifier, due to its ubiquity and wide support by the EUR-Lex website.
The following variables were not of interest for this project and were removed: \texttt{text\_url} (later on we used the CELEX to query the EUR-Lex website), \texttt{doc\_id}, \texttt{api\_url}, \texttt{prelex\_relation}, \texttt{internal\_ref} and \texttt{additional\_info}. We also renamed \texttt{date\_document} to \texttt{date}.

Finally, we converted the metadata to CSV format, which is more useful than JSON in pipelines, because JSON deserialization requires the whole file to be read at once, while CSV can be read one line at a time, although it is not as flexible as JSON. We encoded list fields (\texttt{subject\_matter}, \texttt{directory\_codes} and \texttt{eurovoc\_descriptors}) by joining list items with semicolons, after concluding that none of those fields' items contained semicolons.
The \SI{138911}{} processed entries were stored in \texttt{processed.csv} (\SI{60.6}{\mega\byte})

After exploring the \texttt{addressee} field, we concluded that the \url{api.epdb.eu} API truncated the \texttt{addressee} field to 255 characters. We nevertheless decided to keep this field, as although it has incomplete information, it may still be valuable data.

\subsection{Data filtering}
\label{ssec:filtering}

The additional data filtering step was defined to allow the flexibility required to filter out some documents based on deterministic criteria, in case the information processing and retrieval tool struggled with more data than required for this project. It is currently just a pipe between its input and output, as we did not yet find any issues with the large amount of data in the data processing stage. This left us with a file \texttt{filtered.csv} that is identical to \texttt{processed.csv} in size and contents.

We initially used this processing step to test our initial dataset indexing in section \ref{ssec:data-importing}, but we later realized the system supported the whole dataset.

\subsection{Text collection}

Using the CELEX identifiers from \texttt{filtered.csv}, we constructed URLs with format \path{https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:<}CELEX identifier\texttt{>} to retrieve document contents in HTML format, with the HTML files mostly containing only the document contents, or otherwise some headers. HTTP requests were made to the EUR-Lex website using the Python3 library \texttt{urllib} \cite{urllib}, and the HTML files were converted to plain text by removing all HTML tags with library \texttt{BeaufitulSoup} \cite{beautifulsoup}.

For some documents we were not able to retrieve their contents using the abovementioned URLs. Most such documents did not have a version in English, or instead had a PDF version and no  HTML version. We decided not to handle those files because the vast majority of the \SI{138911}{} files from previous steps had an HTML version in English, and having to consider documents in other languages or parse PDF files would add another layer of complexity when we already had more than enough data for this project.

Each document's contents was stored in a separate \textit{.txt} file named \texttt{texts/<}CELEX identifier\texttt{>.txt}. A complete list of documents for which contents were retrieved is available at \texttt{texts.txt}, one CELEX identifier per line (this file is essentially equivalent to listing all files in directory \texttt{texts} without their extensions). Out of the \SI{138911}{} documents we knew of from previous data processing steps, we were able to find text for \SI{99903}{} ($71.92\%$). The file \texttt{texts.txt} has a size of \SI{1.2}{\mega\byte}, and the total size of the \texttt{texts/} folder containing all document texts we could obtain is \SI{3.3}{\giga\byte}.

\subsection{Data enrichment}

Finally, we processed the text files to remove newlines in cases where there were three or more sequential newlines, and combined the document metadata from \texttt{filtered.csv} with the document contents in \texttt{texts/}, keeping only the documents for which we were able to obtain contents. The result of this operation was stored in \texttt{combined.csv}, with \SI{99903}{} entries and a size of \SI{1.31}{\giga\byte}.

\section{Dataset fields}
\label{sec:dataset-fields}

After completing all steps of data preparation, we obtained a dataset \texttt{combined.csv} with \SI{99903}{} entries and a size of \SI{1.31}{\giga\byte}. It has the fields described in Table \ref{tab:fields}.
The class diagram for the dataset is presented in Figure \ref{fig:class-diagram}.
Dates are represented in the format \texttt{YYYY-MM-DD}, and each date field is zero-padded when needed.

\begin{table}[ht]
    \centering
    \caption{Fields of the final dataset.} \label{tab:fields}
    \begin{tabular}{@{}l|c|l@{}}
        \textbf{Field}               \textbf{Type}   & \textbf{Description}    \\ \hline
        \texttt{celex}                  & text            & CELEX identifier        \\
        \texttt{form}                   & category        & Form of document        \\
        \texttt{date}                   & date            & Date of signature       \\
        \texttt{title}                  & text            & Title                   \\
        \texttt{oj\_date}               & date            & Date of pub. in OJ      \\
        \texttt{of\_effect}             & date            & Date of effect          \\
        \texttt{end\_validity}          & date            & End of validity         \\
        \texttt{addressee}              & text            & Addressee(s)            \\
        \texttt{subject\_matter}        & list            & Subject matters         \\
        \texttt{directory\_codes}       & list            & Directory codes         \\
        \texttt{eurovoc\_descriptors}   & list            & EuroVoc descriptors     \\
        \texttt{legal\_basis}           & list            & IDs of legal basis      \\
        \texttt{relationships}          & list            & IDs of related docs.    \\
        \texttt{text}                   & text            & Content of document     \\
    \end{tabular}
\end{table}

\begin{figure}
  \includegraphics[width=\linewidth]{diagram-class-diagram.drawio}
  \caption{Class diagram for conceptual model.}
  \label{fig:class-diagram}
\end{figure}

\section{Data exploration}
\label{sec:data-exploration}

Data exploration was performed to assert information like common keywords, form types and distribution of document publication date. Data exploration was performed using a Jupyter Notebook, running chunks of Python3 code.

Missing data can be caused by a variety of factors, but it is mostly associated to human error and the evolving metadata needs, which means certain older documents have empty fields, probably because at the time the database did not have all the fields it currently does. Invalid data is originated mostly from platform limitations: if a platform requires a date to have three fields year/month/day but a document validity does not expire, users are forced to use some standardized solution for that case.

\begin{table}[ht]
    \centering
    \caption{Missing values per field.} \label{tab:missing}
    \begin{tabular}{@{}l|r|r@{}}
        \textbf{Field}              & \textbf{Missing}& \textbf{\%} \\ \hline
        \texttt{date}                   & 1 615            & 1.62                    \\
        \texttt{title}                  & 0               & 0.00                    \\
        \texttt{oj\_date}               & 1 971            & 1.97                    \\
        \texttt{of\_effect}             & 14 483           & 14.50                   \\
        \texttt{end\_validity}          & 17 398           & 17.41                   \\
        \texttt{addressee}              & 74 951           & 75.02                   \\
        \texttt{subject\_matter}        & 15 400           & 15.41                   \\
        \texttt{directory\_codes}       & 17 310           & 17.33                   \\
        \texttt{eurovoc\_descriptors}   & 20 386           & 20.41                   \\
        \texttt{legal\_basis}           & 6 567            & 6.57                    \\
        \texttt{relationships}          & 869             & 0.87                    \\
    \end{tabular}
\end{table}

\texttt{addressee} was found to be the column by far with the highest missing data percentage at 75.02\%, as per Table \ref{tab:missing}. This lack of data might be due to the fact that most documents are general European law, and as such are directed at all member states that are part of the EU, regardless of EU membership at the time of publishing.

Column \texttt{end\_validity} has 17.41\% missing elements, which may be due to the indefinite validity of a document. The list columns \texttt{subject\_matter}, \texttt{directory\_codes} and \texttt{eurovoc\_descriptors} range from 15.41\% to 20.41\% missing elements which may be due to the mere absence of list items (e.g., there are no EuroVoc terms associated to the document due to its nature), or due to human error when inputting records in the database, information loss or non-applicability of some fields in certain documents.\par

An interesting observation is that \texttt{relationships} is not present only in 0.87\% of the entries, revealing the strong connection between the vast majority of EU legislation: laws that replace, complement or build upon older ones.

\subsection{Dates}

Document dates range from 24 Sep 1949 to 4 Oct 2013. \texttt{oj\_date} and \texttt{of\_effect} distributions are very similar to \texttt{date}, increasing until 1981 (second EU enlargement). In the 1981-2001 period the number of documents per year grew slowly, until late 2000 when the Nice European Council decided to speed up accession negotiations with the 12 countries that would later join the EU in 2004, causing the number of documents signed in 2001 to increase by almost $60\%$ when compared with 2000. 
This number has stayed somewhat stable and even decreased since 2002. \texttt{end\_validity} values range from 7 Jan 1954 to 31 Dec 2058, with the exception of \SI{34833}{} documents that have as expiration date 1 Jan 2100, which we interpret as the document not expiring.

\begin{figure}[H]
  \includesvg[width=\linewidth]{date-histogram.svg}
  \caption{Date histogram.}
\end{figure}

\subsection{Lists}

The dataset has 249 unique subject matters, being ``common commercial policy'' the one with the highest frequency appearing \SI{11961}{} times, followed by ``External relations'' (\SI{10478}{}) and ``Agriculture'' (8 943).
The dataset uses \SI{5204}{} unique EuroVoc descriptors, with the most frequent being ``import''  (\SI{5871}{}), ``export refund'' (\SI{4785}{}) and ``originating product'' (\SI{3837}{}).
Each document is related to an average of 5.93 other documents. The documents most referred to are \texttt{32007R1234} (regulation on the common organisation of agricultural markets), \texttt{21994A0103(74)} (\textit{Agreement on the European Economic Area}) and \texttt{11957E113} (\textit{Treaty establishing the European Economic Community}).

\subsection{Text fields}

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{title-length-histogram.svg}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{text-length-histogram.svg}
    \end{minipage}
    \caption{Text fields length histograms.}
\end{figure}

Title length varies between 6 and \SI{1531}{} characters, with an average length of 216.3 characters. The title length approximately follows a log-normal distribution, since it is mostly right-skewed and its tail (to the right) tends to get smaller quite slowly.
Text length is between 300 and 6.3M characters, with an average length of \SI{12583.3}{} characters. It presents a distribution similar to \texttt{title}.

\section{Retrieval tasks}
\label{sec:retrieval-tasks}

Here we present some of the information needs we expect to meet:

\begin{enumerate}
    \item Amends to Decision 1999/468/EC
    \item Legislation related to Portugal
    \item Most recent regulation on Agriculture
    \item Competition-related documents in force
    \item Data protection regulation after 2010
    \item Legislation on import-export with Canada
    \item Fishing quotas in the North Atlantic Sea
\end{enumerate}

\section{Indexing}
We decided to use as Information Retrieval tool Solr 8.10 \cite{solrDocs}, given it is free and open software, and its more free text search-oriented approach when compared to other Information Retrieval software. Its workflow consists of importing data, indexing using a provided schema, specifying indexing and querying rules for each field, and finally using its query system to search and extract documents.

\subsection{Data importing} \label{ssec:data-importing}
From the previous stages of development, data was stored in CSV format, which made the importing step straightforward as the Solr endpoint accepts CSV for inserting/updating collections. We only needed to specify the field encapsulator \textit{"}, since fields \textit{title} and \textit{text} included commas, and had to be encapsulated to escape these delimiter characters.
\begin{verbatim} 
    curl -X POST 'http://localhost:8983/solr/docs/update
    /csv?commit=true&encapsulator="' \
    -T /data/combined.csv \
    -H 'Content-type:application/csv'
\end{verbatim}

There was initial concern with the size of the data file and possible issues with using it in the indexing tool. We initially filtered the data in Section \ref{ssec:filtering} to get a file of \textasciitilde\SI{1.8}{\mega\byte}, to assess if Solr was correctly configured and our dataset was being correctly imported and indexed. At a later stage, we switched to the full dataset, which proved to be less troublesome than expected as importing took only \SIrange{100}{200}{\second}.

\subsection{Indexing schema}
Solr indexing configurations are specified in a schema file, which assigns for each field a custom type and a set of operations to be performed on indexing and querying. The schema is uploaded through an endpoint similar to how the data was imported:
\begin{verbatim}
    curl -X POST -H 'Content-type:application/json' \
    --data-binary @/data/schema.json \
    http://localhost:8983/solr/docs/schema
\end{verbatim}

An example of a field type in our schema is as follows:

\begin{lstlisting}[caption={Field type \texttt{docTitle}}]
{
  "name":"docTitle",
  "class":"solr.TextField",
  "indexAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ClassicFilterFactory"},
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  },
  "queryAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ClassicFilterFactory"},
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  }
}
\end{lstlisting}

% This field type is used to index the titles of each document, and the steps it takes are:
% \begin{enumerate}
%     \item \textit{"class"}: the Solr class used to store the index. Ranges from boolean to double point and even date types.
%     \item \textit{"indexAnalyser"}: Used upon ingestion of data to be indexed. Specifies the way indexes will be built and stored. 
%     \item \textit{"queryAnalyser"}: Used in execution of queries and aplied to values being searched for, which are then matched with the stored index's values.
% \end{enumerate}
% Both of these analysers are composed of a tokenizer and a series of filters. The tokenizers are in charge of breaking up the value into tokens, be it by using whitespace and punctuation as delimiters, as is the case of the \textit{StandardTokenizerFactory}, or by a specific set of delimiters, as is the case of the \textit{SimplePatternTokenizerFactory}, that uses regular expressions to match sequences (in our case, in the \textit{docList} type, to split the list of values by semicolons).

Our experiments showed that the default tokenizer and filter configurations are enough for most fields and cases, although some adjustments to the indexing and querying stages proved fruitful. 

A notable example is the \texttt{docText} field type, used to index each document's actual content.
Since this field has newline characters, which are of no value in searches, we use the \textit{PatternReplaceFilterFactory} filter to remove these from the indexing process, as well as a \textit{StopFilterFactory} filter to specify common English words to be ignored (mostly articles and prepositions). The list given in the default \texttt{stopwords.txt} file sufficed for our use case.

\begin{lstlisting}[caption={Field type \texttt{docText}}]
{
  "name":"docText",
  "class":"solr.TextField",
  "large": true,
  "stored": true,
  "multiValued": false,
  "indexAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.PatternReplaceFilterFactory",
        "pattern":"\n", "replacement":" "},
      {"class":"solr.StopFilterFactory",
        "words":"stopwords.txt", "ignoreCase":true},
      {"class":"solr.KStemFilterFactory"}
    ]
  },
  "queryAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  }
}
\end{lstlisting}

We also present the field types of all fields in Table \ref{tab:field-types}.

\begin{table}[ht]
    \centering
    \caption{Field type of each field.} \label{tab:field-types}
    \begin{tabular}{@{}l|l@{}}
        \textbf{Field} & \textbf{Field type} \\ \hline
        \texttt{date}                   & docDate        \\
        \texttt{title}                  & docTitle       \\
        \texttt{oj\_date}               & docDate        \\
        \texttt{of\_effect}             & docDate        \\
        \texttt{end\_validity}          & docDate        \\
        \texttt{addressee}              & docAddressees  \\
        \texttt{subject\_matter}        & docListWords   \\
        \texttt{directory\_codes}       & docList        \\
        \texttt{eurovoc\_descriptors}   & docListWords   \\
        \texttt{legal\_basis}           & docList        \\
        \texttt{relationships}          & docList        \\
        \texttt{text}                   & docText        \\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Underlying data classes for each field type. MV stands for \textit{multivalued}, meaning it is a list.} \label{tab:classes}
    \begin{tabular}{@{}l|l c@{}}
        \textbf{Field type} & \textbf{Class} & \textbf{MV} \\ \hline
        docTitle      & TextField      & N \\
        docText       & TextField      & N \\
        docDate       & DatePointField & N \\
        docAddressees & MV TextField   & Y \\
        docList       & MV TextField   & Y \\
        docListWords  & MV TextField   & Y \\
    \end{tabular}
\end{table}

\section{Queries}

The primary objective of using Solr's query system is to meet the information needs samples presented in Section \ref{sec:retrieval-tasks}. We have translated those information needs into the following queries:

\begin{enumerate}
    \item Amends to Decision 1999/468/EC \begin{verbatim}
title:"1999/468/EC amend"~10^4
text:"1999/468/EC amend"~10
\end{verbatim}
    \item Legislation related to Portugal \begin{verbatim}
text:portugal
title:portugal^4
eurovoc_descriptors:portugal^8
\end{verbatim}
    \item Most recent regulation on Agriculture \begin{verbatim}
text:agriculture
title:agriculture^4
eurovoc_descriptors:agriculture^8
subject_matter:agriculture^8
\end{verbatim}
    \item Competition-related documents in force \begin{verbatim}
text:competition
title:competition^4
eurovoc_descriptors:competition^8
subject_matter:competition^8
\end{verbatim} Filters: \begin{verbatim}
of_effect:[* TO NOW]
end_validity:[NOW TO *]
\end{verbatim}
    \item Data protection regulation after 2010 \begin{verbatim}
title:"data protection"^4~2
eurovoc_descriptors:"data protection"^4~2
text:"data protection"~2
\end{verbatim} Filters: \begin{verbatim}
form:"regulation"
date:["2010-01-01T00:00:00Z" TO *]
\end{verbatim}
    \item Documentation on import-export with Canada \begin{verbatim}
title:"import Canada"^2~5
title:"export Canada"^2~5
text:"import Canada"~10
text:"export Canada"~10
eurovoc_descriptors:"import"^4
eurovoc_descriptors:"export"^4
eurovoc_descriptors:"Canada"^4
\end{verbatim}
    \item Fishing quotas in the North Atlantic Sea \begin{verbatim}
title:"fishing"^2
title:"quotas"^2
title:"North Atlantic"^4~1
text:"fishing"^2
text:"quotas"^2
text:"North Atlantic"^4~1
eurovoc_descriptors:"fishing"^2
eurovoc_descriptors:"quota"^2
eurovoc_descriptors:"North Sea"^4~1
\end{verbatim}
\end{enumerate}

We used the Lucene query parser with query operator set to \texttt{OR} for all queries. Notice that we used filters for some queries.

\section{Result evaluation} \label{sec:evaluation}

\subsection{Relevance analysis} \label{sec:relevance-analysis}

Evaluating recall performance measures requires one to know the complete set of relevant search results among all documents for each query. In our case, it is practically impossible to obtain the relevant sets because our dataset has a considerable size (\SI{99903}{} documents), meaning we would have to judge each of those documents' relevance for each query.

To circumvent this issue, we estimated the relevant sets by assuming that the information retrieval system has a certain minimum quality, and that all documents that are relevant for a certain query will appear in the first $N = 100$ search results for that query. We will from that point on assume those \SI{100}{} documents constitute the whole dataset for that query, and manually evaluate the relevance of each of those documents for said query. This makes the manual relevance judgement more manageable as we have defined 7 sample retrieval tasks, and as such we only have to judge the relevance of at most 700 documents instead of the whole \SI{99903}{} documents.

We will assume that a user of our search system is mostly interested in the $n=10$ most relevant results, as it is the typical number of results in a results page of webpage search engines (such as Google Search), and since the vast majority ($89.69\%$) of Google users only use the first 10 search results, corresponding to the first results page for that search engine \cite{seo-hacker}.

To expedite the manual relevance judgement and calculation of performance measures, a small tool was developed. This is a monolithic HTML document with included CSS and Javascript code, which allows anyone to:
\begin{itemize}
    \item Import the Solr query results in JSON format
    \item Display those results in a user-friendly way for manual inspection and relevance judgement
    \item Easily select relevant documents
    \item Export the list of relevant documents
    \item See the most common performance measures, which are automatically calculated and updated based on user input
\end{itemize}

\subsection{Performance measures}

We used the abovementioned HTML document to automatically get the performance measures after manual relevance judgement. We calculated the following measures for all queries \cite{search-engines-2015}:

\begin{itemize}
    \item Precision at $n$ (P@n): fraction of the $n$ first search results that are relevant.
    \begin{equation}
        P_n = \frac{TP_n}{TP_n + FP_n}
    \end{equation}
    \item Recall at $n$ (R@n): fraction of relevant documents in the dataset which are in the $n$ first search results.
    \begin{equation}
        R_n = \frac{TP_n}{TP_n + FN_n}
    \end{equation}
    \item F-measure at $n$ (F@n): a score that balances recall and precision for a query showing $n$ results using a formula similar to a geometric mean. We are using $\beta=1$ until further thought is put into which criteria is more important in our use case, if any.
    \begin{equation}
        F_n = (1 + \beta^2)\frac{P_n \times R_n}{\beta^2 \times P_n + R_n}
    \end{equation}
    \item Average Precision (AvP): arithmetic average of the values of P@n for values $n$ where relevant documents occur, for $1 \leq n \leq N$, where $N$ is the dataset size (which, due to the approximations mentioned in Section \ref{sec:relevance-analysis}, is at most $N=100$).
    \begin{equation}
        AvP = \frac{1}{\text{\#relevant}} \sum_{i~\text{relevant}}{P_i}
    \end{equation}
    \item Mean Average Precision (MAP): arithmetic mean of the AvP for a set of $Q$ queries using the same information retrieval system, used as a global system performance score.
    \begin{equation}
        MAP = \frac{1}{Q} \sum_{0 \leq q < Q}{AvP_q}
    \end{equation}
\end{itemize}

We calculated P@n, R@n and F@n for all 7 queries and all valid values of $n$ for each query (e.g., some queries returned less than $N=100$ results) and AvP for all queries, presented in Table \ref{tab:ir-quality}. Since $n=10$ was considered the most relevant number of search results, we only show P@10, R@10 and F@10. Further system performance information is shown in Figure \ref{fig:recall-precision-graphs}.

\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{recall-precision.svg}
    \includesvg[width=\linewidth]{recall-precision-interpolated.svg}
    \caption{Precision-recall graphs.} \label{fig:recall-precision-graphs}
\end{figure}

We also calculated the value of MAP, which is 0.68 for the system at its current iteration.

\begin{table}[ht]
    \centering
    \caption{Information retrieval effectiveness measures.} \label{tab:ir-quality}
    \begin{tabular}{@{}l|r r r r@{}}
        \textbf{Query}   & AvP  & P@10 & R@10 & F@10 \\ \hline
        Query 1          & 0.50 & 0.40 & 0.57 & 0.47 \\
        Query 2          & 0.85 & 0.80 & 0.10 & 0.17 \\
        Query 3          & 0.30 & 0.40 & 0.13 & 0.20 \\
        Query 4          & 1.00 & 1.00 & 0.10 & 0.18 \\
        Query 5          & 0.55 & 0.50 & 0.50 & 0.50 \\
        Query 6          & 0.93 & 1.00 & 0.14 & 0.25 \\
        Query 7          & 0.60 & 0.70 & 0.12 & 0.21 \\
    \end{tabular}
\end{table}

These results reveal that the search system is working decently, with some queries giving almost-perfect results (Q2, Q4, Q5), awful (Q3) and reasonable (Q1, Q6, Q7). The problems revealed by the queries with the worst scores were considered and reflected on possible future work.

\section{Future work}

We intend to use other query parsers, namely \textit{DisMax} and \textit{eDisMax}, which will be used to assess possible improvements in query results quality.
We also intend to implement a simple front-end interface for the search system, although it is not a priority compared to the following improvements.

\subsection{Related documents}
On all queries, the ability to show the top related documents, which would be implemented in the following way: Execute the input query, and then do another query but the 10-best ranked CELEX from the first are collected, and documents `related' to those 10 best-ranked CELEX (i.e., present in their \texttt{relationships} field) are weighted more on the 2nd query; the final results are those of the 2nd query. 
    
In practice, this would show the top 10 documents from the input query along with all the documents reference by those. This could be visually improved using grouping, though that is still an effort to be explored.

\subsection{Faceting}
Making use of faceting on fields with tag-like structure such as \texttt{directory\_codes}, \texttt{eurovoc\_descriptors} and \texttt{addressee} to arrange search results into groups would complement the use of queries, since it would show the specific tags already in the documents, without the need for the user to guess the correct terminology.

\subsection{Synonyms in queries}
Another feature that could be of aid is the use of the \textit{SynonymGraphFilter} in the query phase, to spread out the terms that encompass the query. This, however, has the limitation of the size of the synonym list to input into the system, since our database encompasses a wide variety of topics which would require an extensive list of synonyms, which may not scale well for our application. We will pursue the faceting option first, and test the synonyms approach if performance is not significantly affected and if we have time.

\subsection{Date as scoring criteria}
Say we want the regular scoring to be performed based on token occurrence, but we would prefer that more recent documents come up first. We don't want to sort results by date, because if we're searching for a very common keyword we'll get many documents that are from the last year we collected data for; instead, we would like to use both date and score for ranking, so a reasonably old but very well-scored document comes up at the top, and very recent documents without a decent score come up towards the end.

\subsection{Page rating}
Implement a document-rating algorithm. If we're searching about a generic topic of interest, there will be many document matches, many of which will be about a specific topic somehow related to our topic of interest, which got well scored just because they incidentally mention our topic of interest very often. We would rather prefer to better rank documents based on the following characteristics, which can be fully or partially computed \textit{a priori}.

Consider the following documents to be used in the examples below:
\begin{itemize}
    \item \textbf{D1}: \texttt{32007R1234} -- \textit{Council Regulation (EC) No 1234/2007 of 22 October 2007 establishing a common organisation of agricultural markets and on specific provisions for certain agricultural products}
    \item \textbf{D2}: \texttt{32013R0849} -- \textit{Commission Implementing Regulation (EU) No 849/2013 of 2 September 2013 establishing the standard import values for determining the entry price of certain fruit and vegetables} (consider as a simplification that this document is about the transport conditions of oranges in the EU)
\end{itemize}

We will assume we are doing a search for ``agriculture'', and that both documents mention ``agriculture'' about as often.

\subsubsection{Specificity}

We find D1 more relevant for this search, because ``agriculture'' is one of the few keywords of D1, while D2 mentions ``agriculture'' as often but also mentions some uncommon topics like ``produce transport'', ``citrus'' and ``oranges'', in increasing order of specificity.

When asked to answer the question \textit{What are these two documents about?}, a bad answer is that both are about agriculture (based on mere occurrence counting), and a good answer is that D1 is about ``agriculture'' and D2 is about ``oranges'', because even if D2 may not refer to ``oranges'' as much as it does to ``agriculture'', ``oranges'' is a much rarer and more specific topic than ``agriculture'', and must be weighed in as such.

Thus, if we're searching for ``agriculture'', we want documents that mention ``agriculture'' along many other topics and more specific topics to be lower in the ranking, and documents that almost only mention ``agriculture'' (even if not that often) should be ranked higher in the ranking.

\subsubsection{Importance} \label{sec:futurework-importance}

If two documents are tied using some criteria, we would like to get first the document that is more \textit{important}. This is obviously an ambiguous concept, but for this effect we can consider a document \textit{more important} than another if, when those two documents are presented side-by-side, someone would rather open one document first than the other, because that person believes one of them is considered to have greater authority on that issue, or because that document is referenced more often in other documents.

If we choose to stick with the latter definition of importance measured by references, we are left with a classical page-rating problem where we only know the structure of the page graph. We can employ such scoring system because fields \texttt{legal\_basis} and \texttt{relationships} provide us with connections between documents.

\section{Improvements}
\subsection{Query parser comparison}

To allow a fair comparison of the three query parsers available in Solr, we chose a 'middle-of-the-pack-performing' query: (7) Fishing quotas in the North Atlantic Sea. We evaluated this query using the three available query parsers on Solr: Lucene \cite{lucene}, DisMax \cite{dismax} and Extended DisMax (eDisMax) \cite{edismax}.
We used the following parameter values for DisMax and eDisMax (\texttt{ps}, \texttt{bq}, \texttt{pf2}, \texttt{pf3}, \texttt{ps2} are exclusive to eDisMax):
\begin{verbatim}
qf: title^5 eurovoc_descriptors^5 subject_matter^5 text
ps: 1
bq: title:"fishing"^2 title:"quotas"^2
    title:"North Atlantic"^3 text:"fishing"^2
    text:"quotas"^2 text:"North Atlantic"^3
    eurovoc_descriptors:"fishing"^2
    eurovoc_descriptors:"quota"^2
    eurovoc_descriptors:"North Sea"^3
    subject_matter:"fishing"^2
    subject_matter:"quota"^2
    subject_matter:"North Sea"^3
pf2: title^3 eurovoc_descriptors^3 subject_matter^3
     text
pf3: text
ps2: 2
\end{verbatim}


\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{recall-precision-query-parsers.svg}
    \includesvg[width=\linewidth]{recall-precision-interpolated-query-parsers.svg}
    \caption{Precision-recall graphs for a selected query using the Lucene, DisMax and eDisMax query parsers.} \label{fig:recall-precision-graphs-query-parsers}
\end{figure}

\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{f-measure-query-parsers.svg}
    \caption{F-measure graphs for a selected query using the Lucene, DisMax and eDisMax query parsers.} \label{fig:f-measure-graphs-query-parsers}
\end{figure}

As shown in Figure \ref{fig:recall-precision-graphs-query-parsers}, overall, DisMax and eDisMax both represent an improvement over their Lucene counterpart, having Average Precision of 0.64, 0.73 and 0.60, respectively. Between them, eDisMax has the edge which is clearly evidenced when combining precision with recall (F-measure). We have decided to use eDisMax for all queries from this point on.

\subsection{Frontend}
The first feature in the form of an improvement was the creation of a user friendly interface for the search tool. With a layout based on most similar tools, it features a free-text search bar on top and a sidebar for finer filtering through \hyperref[sec:faceting]{faceting}. This frontend communicates directly with the Solr backend, using both eDismax for free-text search and Lucene to apply scrict searches (by tags, by CELEX, etc.).

The document metadata is displayed beside the text and allows for easier and faster assertion of relevance of a result.

The main motivation behind implementing a frontend was accurately representing data, so we decided to display the document body as closely as possible to the original form. By storing document content in the original \textit{rich text} format with no processing, we can render the HTML as intended, and redirect requests of other EUR-Lex resources (CSS, JS, images) directly to the EUR-Lex website. The result is a representation that facilitates the evaluation of relevance, all the while keeping the original content and format for unadulterated consultation.

\subsection{Related documents}
Our initial approach to related documents consisted of compositing the results of two queries in order to collect each result's relationships and boost their scores accordingly. This proved cumbersome due to the nature of our data. Related documents may be of interest if a user is focused on a specific document, but they're likely not of much use for an informational search just because they are related to the best-scored documents of a first query.

We opted for a simple but effective alternative: a simple link on each document that executes a query for that document's relatives. Though not as complex and interesting as our former idea, it still proved useful in the scope of navigating for a specific set of the legislature through document relationships, which was the desired result on the topic of taking advantage of relationships.

\subsection{Faceting}
\label{sec:faceting}
Stemming from the confusing phrasing of title and text in our dataset, there was a need to filter out results in a more controlled way than through the free-text field to make the results list more manageable. Since our documents featured various fields with tag-like structure, using faceting for this functionality was the most effective approach.

After a user runs a query, they are presented with a list of different values for addressee, Eurovoc descriptors, subject matters and legal basis that appeared in the documents resulting from the query, as well as a count for each tag. The user can then select a few of these by hand for faceting. For example, ticking Eurovoc descriptor 'health control' and addressee 'Sweden' reduced the number of results from from 10128 to 4.

The performance of this feature lies almost entirely on the correctness of the dataset: if fields are correctly and thoroughly filled in. This means it can fail when the user is looking for older, incomplete documents.

One problem that we faced when developing this feature was the data type of the fields to be faceted. By definition, these fields should not be tokenized or processed in the indexing phase, since usually they are present only for faceting purposes. However, in our system, we wanted to keep the ability for free-text search in these fields with larger weights, and as such, had to tokenize them. This proved troublesome when filtering by obtained faceting, since multi-word values weren't treated as a single phrase in the \texttt{fq} field, and so a workaround for these cases had to be used. Namely, the separator for words in a term became \texttt{*} and the separator for terms became \texttt{AND}, replacing white-space.

\subsection{Synonyms}

The inclusion of synonyms was done using \textit{SynonymGraphFilterFactory} \cite{synonym-graph-filter-factory}. This filter was only added on the query side, as using this filter while indexing did not significantly improve result quality as far as we could perceive. Besides, this change represented an huge increase in indexing time from $\SI{3.5}{\min}$ to $\SI{50}{\min}$.

This filter requires providing a \texttt{synonyms.txt} file, which contains a list of synonyms in a syntax specific to Solr. To generate a \texttt{synonyms.txt} file we used WordNet, a lexical database of English \cite{wordnet}; we obtained the ANSI Prolog version of WordNet 3.0. We then used a convenience Java class \texttt{Syns2Syms} \cite{syns2syms} to convert the WordNet synonyms file \texttt{wn\_s.pl} into a \texttt{synonyms.txt} file that the Solr filter could parse. Upon visual inspection of the resulting \texttt{synonyms.txt} file, we concluded it was correctly formatted and would suffice at least in providing a large set of general-purpose synonyms.

To assess the correct functioning of the synonyms feature, we decided to evaluate query \texttt{vibramycin} (one of the brand names for doxycycline, a broad-spectrum antibiotic). Using the system without synonyms, 0 results were returned. When synonyms were added, the query returned 16 results, all 16 of which were considered relevant because they all mentioned \textit{doxycycline}.

% \subsection{Date as scoring criteria}

% TODO

\subsection{Page rating}

We addressed the challenge of finding the importance of a page by considering references between documents as described in Section \ref{sec:futurework-importance}. We used the PageRank algorithm \cite{pagerank} to assign a score to each page as a measure of importance. This algorithm considers links as references between pages and assumes that a page linking another page \textit{endorses} it: being endorsed more often indicates importance, and being endorsed by many important pages indicates even more importance. The PageRank algorithm models a random surfer which is always in a certain page, and has a probability $\alpha$ (\textit{damping factor}) of following any link of that page at random, and probability $1-\alpha$ of jumping to a random page. The PageRank score of a page is the relative frequency of visits to that page.

This score can be determined by a Monte Carlo simulation, but the following iterative process converges faster. The network (or in this case, the dataset) is abstracted into a directed graph where a page is a node and a link from page $u$ to $v$ is an edge $(u,v)$ in the set of edges $E$. Let $out$ be the out-degree of node $u$.

\noindent
\begin{equation}
    p(v)_t = \begin{cases}
        \displaystyle\frac{1}{N} &: t=0 \\[8pt]
        \displaystyle\frac{1-\alpha}{N} + \alpha \sum_{u:~(u,v) \in E}{\frac{p(u)_{t-1}}{out(u)}} &: t>0
    \end{cases} \label{eq:iterative}
\end{equation}

\noindent
The score of page $i$ is $\lim_{t \to \infty}{p(i)_t}$, but we can use an error margin $\varepsilon$ as stopping criteria.

Another way of calculating the score, known as the power method, is based on the realization that the recursive step can be reinterpreted in the matrix form

\noindent
\begin{gather}
    \matr{M} = \alpha \matr{T} + \frac{1-\alpha}{N}\matr{J}_N \\[4pt]
    \vec{p} = \matr{M} \vec{p} \label{eq:eigenvector}
\end{gather}

\noindent
where $\vec{p}$ is the score vector, $\matr{T}$ is the transition matrix as defined for Markov chains ($\matr{T}_{ij}={out(i)}^{-1}$ if $i$ links to $j$ and $out(i) > 0$, or $0$ otherwise), and $\matr{J}_N$ is an $N \times N$ matrix of all-ones. We can easily calculate $\matr{M}$, and given the constraints of this problem and Equation \ref{eq:eigenvector}, $\vec{p}$ is the principal eigenvector of $\matr{M}$, which can be calculated using the power method, an iterative method of finding the largest eigenvector of a matrix. The power method can also use an error margin $\varepsilon$ as stopping criteria.

We will not go into much detail about convergence, but there are two important points to be made. The power method of finding an eigenvector does not give a convergence guarantee, but the PageRank algorithm with $0 < \alpha < 1$ can be modeled as the problem of finding the stationary distribution of a discrete irreducible Markov chain, for which it has been proven that the stationary distribution can be calculated by iteratively applying Equation \ref{eq:iterative} or \ref{eq:eigenvector}, because this process always converges to a single solution regardless of the initial scores.

We used Python library NetworkX \cite{networkx}, specifically function \texttt{pagerank} \cite{networkx-pagerank} with parameters $\alpha=0.85$ and $\varepsilon=10^{-19}$. This implementation uses the power method to determine the PageRank scores of nodes in a directed graph. We added a new step to our data processing pipeline, after obtaining the texts and before combining all data, to calculate the PageRank of every document with text.
% For our dataset, the PageRank converged in 174 iterations.

Most use cases (including the original PageRank report) consist of obtaining matching entries from the dataset, and then use PageRank to sort the matching entries. However, Solr calculates an elaborate score based on several factors related to the query text. We could simply ignore the Solr score and sort by importance, but we would be missing very important information that Solr has already evaluated for us. As such, an ideal solution would be to combine importance with Solr score.

The PageRank score has an odd distribution because it is not normalized, and because of inconsistency between the distributions of the most and least important documents. This can be seen by the fact that the average value of $p$ is $1.001 \times 10^{-5}$ and its standard deviation is $6.858 \times 10^{-5}$, Table \ref{tab:pagerank}, meaning larger values are orders of magnitude greater than the mean. This oddity is confirmed by Figure \ref{fig:pagerank-lorenz}, where the Lorenz curve for $p$ has a reduced area, which points towards a large Gini coefficient, Table \ref{tab:pagerank}, and as such high inequalities of PageRank score among documents.

\begin{table}[t]
    \centering
    \caption{Distribution parameters of the PageRank score $p$ and the importance score $p'$.} \label{tab:pagerank}
    \begin{tabular}{@{}l|r|r@{}}
                & \textbf{$p$}              & \textbf{$p'$} \\ \hline
        mean    & $1.001 \times 10^{-5}$    & 0.324         \\
        std     & $6.858 \times 10^{-5}$    & 0.109         \\
        min     & $1.540 \times 10^{-6}$    & 0.001         \\
        median  & $5.71 \times 10^{-6}$     & 0.317         \\
        max     & $1.082 \times 10^{-2}$    & 0.999         \\
        gini    & 0.502                     & 0.175         \\
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{rank-histogram.svg}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{rank-expr-histogram.svg}
    \end{minipage}
    
    \caption{Histograms of Pagerank score $p$ and the importance score $p'$.} \label{fig:pagerank}
\end{figure}

\begin{figure}[t]
    \centering
    \includesvg[width=0.7\linewidth]{rank-lorenz.svg}
    \caption{Lorenz curves of PageRank score $p$ and importance score $p'$.} \label{fig:pagerank-lorenz}
\end{figure}

We intend to obtain a variable $p'$ in range $[0, 1]$ representing the importance of a document, and multiply the Solr score by $1 + p'$, which is in range $[1, 2]$.
We devised a formula to convert PageRank scores into a variable with more suitable range and distribution.
For each document with PageRank score $p$, we applied formula $\ln{(\ln{p}+15)}$ to get a distribution that was not so dispersed, so as to approximate the most and least important documents. We normalized this result to the $[0, 1]$ range. The final formula is:

\noindent
\begin{equation}
    p' = (\log_{10}{(\log_{10}{p} + 6.5)} + 0.163)/0.820 \label{eq:importance}
\end{equation}

The distribution parameters are available in Table \ref{tab:pagerank}, and the histograms in Figure \ref{fig:pagerank}. We managed to get an importance score with more reasonable parameters and Lorenz curve, Figure \ref{fig:pagerank-lorenz}.

We would like to reiterate the fact that this formula was devised in a context-specific way, with the generic goals of reducing the value range and obtaining an importance distribution that we deemed suitable. As such, we cannot provide any proof or scientific methodology that justifies the formula we are using, aside from the relatively lacking explanations given above.

We translated Equation \ref{eq:importance} to the following expression to be used in the eDisMax field \texttt{boost}, corresponding to an expression to be multiplied to the score from the main query \cite{edismax}. As stated earlier, we will multiply the Solr score by $1+p'$.

\begin{verbatim}
sum(1,div(sum(log(sum(log(rank),6.5)),0.163),0.82))
\end{verbatim}

\section{Result re-evaluation}

We re-evaluated the queries evaluated in Section \ref{sec:evaluation}, using the same methods. For all queries, we used eDisMax and the following configurations:

\begin{verbatim}
qf:
  title^5 eurovoc_descriptors^5 subject_matter^5 text
boost:
  sum(1,div(sum(log(sum(log(rank),6.5)),0.163),0.82))
\end{verbatim}

We saw mixed results when comparing the performance measures before and after the improvements. The MAP after improvements is 0.68, which is the same value as before improvements within two decimal places. The values of performance measures AvP, P@10, R@10 and F@10 show a mixed trend which contributed to the similarity of MAP values, Table \ref{tab:ir-quality-new}.

The results of the interpolated recall-precision graphs after improvements, Figure \ref{fig:recall-precision-graphs-new}, seem to show better results than before improvements, as the recall-precision curves tend to start at higher precision values, as well as sustaining higher precision values as recall increases. As mentioned before, this seemingly better trend is not reflected in the performance measures.

\begin{table}[ht]
    \centering
    \caption{Information retrieval effectiveness measures after improvements, and difference to measures before improvements (in parenthesis).} \label{tab:ir-quality-new}
    \begin{tabular}{@{}l | r@{\hspace{3pt}}r | r@{\hspace{3pt}}r | r@{\hspace{3pt}}r | r@{\hspace{3pt}}r@{}}
        \textbf{Query}   & \multicolumn{2}{c|}{AvP} & \multicolumn{2}{c|}{P@10} & \multicolumn{2}{c|}{R@10} & \multicolumn{2}{c}{F@10}  \\ \hline
        Query 1          & 0.45 & (-0.05) & 0.50 & (+0.10) & 0.56 & (-0.01) & 0.53 & (+0.06) \\
        Query 2          & 0.77 & (-0.08) & 0.80 & (=)     & 0.10 & (=)     & 0.17 & (=)     \\
        Query 3          & 0.83 & (+0.53) & 1.00 & (+0.60) & 0.14 & (+0.01) & 0.25 & (+0.05) \\
        Query 4          & 0.70 & (-0.30) & 0.60 & (-0.40) & 0.09 & (-0.01) & 0.16 & (-0.02) \\
        Query 5          & 0.22 & (-0.33) & 0.30 & (-0.20) & 0.18 & (-0.32) & 0.22 & (-0.28) \\
        Query 6          & 0.87 & (-0.06) & 0.80 & (-0.20) & 0.12 & (-0.02) & 0.21 & (-0.04) \\
        Query 7          & 0.89 & (+0.29) & 0.90 & (+0.20) & 0.11 & (-0.01) & 0.19 & (-0.02) \\
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    % \includesvg[width=\linewidth]{recall-precision-new.svg}
    \includesvg[width=\linewidth]{recall-precision-interpolated-new.svg}
    \caption{Precision-recall graphs after improvements.} \label{fig:recall-precision-graphs-new}
\end{figure}

\subsection{Analysis}

The greatest obstacle towards a significant increase in performance after improvements was the poor implementation of synonyms, which ended up counteracting earlier improvements that possibly increased system performance (changing parser to eDisMax, as well as implementing PageRank). This is because synonyms in Solr cannot be handled in a flexible way, so that a query being expanded with each word's synonyms causes the following issues:

\begin{itemize}
    \item The global meaning of the expression is lost, because multi-word synonyms are not easily handled with Solr, so the query ends up being expanded with words that are not related to the global semantic meaning of the query.
    \item Although synonyms are somewhat important in improving performance, the original wording of the query is expected to be the most semantically correct interpretation of the query, because it was the user's choice of words. This means the original query words should have a greater impact in the score, although Solr considers the original word and its synonyms as equals.
\end{itemize}

It is also possible that the change from Lucene to DisMax/eDisMax was partially responsible for these lacking results. Lucene is a query parser designed for more technical/analytical search because a user must specify which words to search in each field individually. On the other hand, DisMax and eDisMax are better suited for free-text search, which is the most common paradigm for contemporary search engines. That being said, our transition from Lucene to DisMax/eDisMax was practically mandatory to meet user expectations of a free-text search engine. There is an obvious problem with DisMax and eDisMax: we either (1) configure parameters \textit{a priori} and use the same configuration for all queries, or (2) we create a meta-search engine that engineers the best value for each parameter by analyzing each query. Because the second option is impractical, we optimized the parameters at our disposal as much as possible to find a configuration that would be a good balance for all queries. This problem does not exist if using Lucene, where we overfitted the query parameters to some degree that was practically impossible to improve on with a single eDisMax configuration for all queries.

\section{Conclusion}

The fundamental goals of this project were to search and select relevant datasets, perform data exploration analysis, assess the datasets' quality, characterize its fields and identify information needs for the system to be developed. We were further expected to index our dataset using an information retrieval tool and perform basic querying tasks, as well as evaluate the returned results' relevance.
% On the third and final iteration, we are expected to improve upon this system and finalize the product by making use of features and techniques with the goal of improving the quality of the search results.

From the EUR-Lex database, we obtained large amounts of data from which we chose a subset of documents with data as coherent and complete as possible. We indexed the dataset using Solr, formulated, tested and evaluated the results for 7 queries (based on previously defined information needs), and manually assessed the results for these in terms of relevance. We evaluated different performance metrics for the queries, showing decent, good, or very good scores.

We improved the system by using a query parser more suitable for free-text search, implementing a front-end interface, and adding synonyms and page rating for discerning documents by importance. We reevaluated the 7 queries, having obtained similar results, presumably due to improper usage of synonyms caused by Solr limitations.

\bibliographystyle{ACM-Reference-Format}
\bibliography{\mainabsdir/../bib/bibliography-file.bib}

\end{document}
\endinput
