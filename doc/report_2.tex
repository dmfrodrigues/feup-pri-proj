\documentclass[sigconf, authorversion]{acmart}

\usepackage{svg}
\usepackage{hyperref}
\usepackage[]{siunitx}
\PassOptionsToPackage{obeyspaces}{url}
\usepackage{dblfloatfix}

\usepackage{listings}

\lstset{
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    frame=tb, % draw frame at top and bottom of the code
    tabsize=2, % tab space width
    numbers=left, % display line numbers on the left
    showstringspaces=true, % don't show spaces in strings
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\urlstyle{tt}

\settopmatter{printacmref=false}

\graphicspath{{../img/}}

\usepackage{currfile-abspath}

\sisetup{
    group-separator = {\,},
    range-units=single,
    range-phrase=--,
    group-minimum-digits=3
}

\getmainfile % get real main file (can be different than jobname in some cases)
\getabspath{\themainfile} % or use \jobname.tex instead (not as safe)
\let\mainabsdir\theabsdir % save result away (macro will be overwritten by the next \getabspath
\let\mainabspath\theabspath % save result away (macro will be overwritten by the next \getabspath

\begin{document}

\title{Information Retrieval System for European Union Legislation}

\author{Diogo Miguel F. Rodrigues}
\email{up201806429@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\author{João António C. V. B. Sousa}
\email{up201806613@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\author{Rafael Soares Ribeiro}
\email{up201806330@edu.fe.up.pt}
\affiliation{%
  \department{M.EIC}
  \institution{Faculty of Engineering of the University of Porto}
  \city{Porto}
  \country{Portugal}
}

\renewcommand{\shortauthors}{Rodrigues, Sousa and Ribeiro}

\begin{abstract}
    EUR-Lex is an European legislation database that offers access to European Union (EU) law,
    case-law by the Court of Justice of the EU and other public EU documents.
    This project aims to retrieve, process and prepare the data from this database, in order to create an information retrieval system of EU legislation.
    Various methods were used to achieve this, from the data collection and filtering phase, to the exploration of the useful data and finally its application in a search system.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317</concept_id>
<concept_desc>Information systems~Information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003381.10003382</concept_id>
<concept_desc>Information systems~Structured text search</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}
\ccsdesc[500]{Information systems~Structured text search}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, information processing, information retrieval, full-text search}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\maketitle

\section{Introduction}
\label{intro}
The European Union (EU) is an economic and political union of 27 member states of Europe. Its predecessors were the European Community for Coal and Steel (ECCS) and the European Economic Community (ECC), created in 1951 and 1958 in the wake of the Second World War. The EU maintains the fundamental goals the ECCS and ECC had of promoting peace and economic cooperation among countries so that a new war among European nations would be economically infeasible. The EU has over time expanded its objectives so as to protect what has been known as the \textit{European Values}, encoded into the Charter of Fundamental Rights \cite{cfr} and the founding treaties of the EU \cite{teu, tfeu}.
The Commission has the initiative for legislation, citizens are entitled to participate in the shaping of the EU legislation through the election of the European Parliament, and their respective state governments through the European Council, being the European Court of Justice responsible for enforcing EU law.

EU law is a system of rules operating within the EU member states, and covers topics from Constitutional and Administrative Law to Freedom of Movement, Trade and Public Regulation, Natural Resource Management and Social Market Regulations.
The three main European legislation databases are EUR-Lex, PreLex and OEIL. EUR-Lex contains information of all the documents of the Official Journal (OJ) of the EU starting at the year of 1951, PreLex has complete records starting at 1974 and is an extension of EUR-Lex that has many of the same documents, but focuses on inter-institutional procedures and data concerning different stages in the decision-making process and law that has not been settled yet, meaning it is most likely to contain drafts of EU legislation. Lastly, OEIL is the database of the European Parliament and stores inter-institutional decision-making in the EU but from the point of view of the Parliament.

\section{Aspects of European legislation}

\begin{figure*}[b]
  \includegraphics[width=\textwidth]{diagram-pipeline.drawio}
  \caption{Data processing pipeline.}
\end{figure*}

\subsection{Identifier systems}

Most documents in EUR-Lex are assigned a unique, language independent CELEX (\textit{Communitatis Europae Lex}) number \cite{celex}. It has the format: Sector number -- Year (4 digits) -- Doc. type -- Doc. number.
ECLI (European Case-Law Identifier) is a uniform identifier used primarily in judicial decisions \cite{ecli}.
ELI (European Legislation Identifier) is a URI that can be read and used by humans and computers to refer to legislation. It is mostly associated with the OJ, and is endorsed by the European Council \cite{eli}.

\subsection{Classification systems}

The institutions of the EU use three classification systems under which they classify documents: the Directory of European Union Legislation, EuroVoc and subject matter.

The Directory of European Union Legislation (or otherwise \textit{directory codes}) is a numeric classification system used primarily by EUR-Lex. It assigns each area/sub-area of interest a number, and each area may contain many sub-areas; a directory code is a dot-separated list of area numbers. For example, a document related to agricultural research has directory code \texttt{03.30.50}: \textit{Agriculture} (\texttt{03}) $\rightarrow$ \textit{Agricultural structures} (\texttt{03.30}) $\rightarrow$ \textit{Agricultural research} (\texttt{03.30.50}) \cite{directory-legal-acts}.

The EuroVoc is a multilingual, multidisciplinary thesaurus covering the activities of the EU. It contains terms in 23 EU languages and 3 languages of candidate EU members, and is used by European institutions, the Publications Office of the EU, national and regional parliaments in Europe, and governments and private users around the world \cite{eurovoc}.

The subject matter authority table (AT) is a controlled vocabulary containing the concepts used for the indexation of notices published on EUR-Lex. It differs from directory codes and the EuroVoc in that the AT is strictly aligned with the evolution of EU policies cited in the different treaties of the EU \cite{subject-matter-at}.

\section{Dataset preparation}

\subsection{Metadata collection} \label{ssec:metadata-collection}

The dataset selected for this project was the EUR-Lex database. By making use of the \url{api.epdb.eu} API (namely of the \url{api.epdb.eu/eurlex/document} endpoint), we extracted a list of \SI{138911}{} document metadata entries, which we stored in file \texttt{raw.json}, with a size of \SI{387.8}{\mega\byte}.

For each document, the following fields were obtained:

\begin{itemize}
    \item \texttt{form}: Form (e.g., Agreement, Recommendation, ...)
    \item \texttt{date\_document}: Document date; generally date of signature.
    \item \texttt{title}: Document title.
    \item \texttt{oj\_date}: Date of publication in the OJ.
    \item \texttt{of\_effect}: Date of effect.
    \item \texttt{end\_validity}: Date the document validity ends.
    \item \texttt{addressee}: Addressee of the document (e.g., countries to which the document concerns).
    \item \texttt{subject\_matter}: Subject matters the document concerns.
    \item \texttt{directory\_codes}: List of directory codes.
    \item \texttt{eurovoc\_descriptors}: List of EuroVoc terms.
    \item \texttt{legal\_basis}: List of EU legislation identifiers (mostly CELEX IDs) that forms the legal basis of the current document.
    \item \texttt{relationships}: List of relationships between this document and other EUR-Lex documents (amendments, legal basis, ...).
    
    \item \texttt{eurlex\_perma\_url}: Permanent link of the document in the EUR-Lex website.

    \item \texttt{text\_url}: URL where document content is available.

    \item \texttt{doc\_id}: Document ID in the API.
    \item \texttt{api\_url}: URL of the document in the API
    \item \texttt{prelex\_relation}: List of documents in the PreLex database related to the current document.
    \item \texttt{internal\_ref}: Internal reference number of the document in the responsible body's separate database.
    \item \texttt{additional\_info}: Mostly used when the document has complex rules for start/end of validity.
\end{itemize}

\subsection{Data cleaning}
\label{ssec:data-cleaning}

After obtaining the metadata, we parsed \texttt{eurlex\_perma\_url} to obtain the CELEX identifiers. We decided to use CELEX as the primary document identifier, due to its ubiquity and wide support by the EUR-Lex website.
The following variables were not of interest for this project and were removed: \texttt{text\_url} (later on we used the CELEX to query the EUR-Lex website), \texttt{doc\_id}, \texttt{api\_url}, \texttt{prelex\_relation}, \texttt{internal\_ref} and \texttt{additional\_info}. We also renamed \texttt{date\_document} to \texttt{date}.

Finally, we converted the metadata to CSV format, which is more useful than JSON in pipelines, because JSON deserialization requires the whole file to be read at once, while CSV can be read one line at a time, although it is not as flexible as JSON. We encoded list fields (\texttt{subject\_matter}, \texttt{directory\_codes} and \texttt{eurovoc\_descriptors}) by joining list items with semicolons, after concluding that none of those fields' items contained semicolons.
The \SI{138911}{} processed entries were stored in \texttt{processed.csv} (\SI{60.6}{\mega\byte})

After exploring the \texttt{addressee} field, we concluded that the \url{api.epdb.eu} API truncated the \texttt{addressee} field to 255 characters. We nevertheless decided to keep this field, as although it has incomplete information, it may still be valuable data.

\subsection{Data filtering}
\label{ssec:filtering}

The additional data filtering step was defined to allow the flexibility required to filter out some documents based on deterministic criteria, in case the information processing and retrieval tool struggled with more data than required for this project. It is currently just a pipe between its input and output, as we did not yet find any issues with the large amount of data in the data processing stage. This left us with a file \texttt{filtered.csv} that is identical to \texttt{processed.csv} in size and contents.

We initially used this processing step to test our initial dataset indexing in section \ref{ssec:data-importing}, but we later realized the system supported the whole dataset.

\subsection{Text collection}

Using the CELEX identifiers from \texttt{filtered.csv}, we constructed URLs with format \path{https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:<}CELEX identifier\texttt{>} to retrieve document contents in HTML format, with the HTML files mostly containing only the document contents, or otherwise some headers. HTTP requests were made to the EUR-Lex website using the Python3 library \texttt{urllib} \cite{urllib}, and the HTML files were converted to plain text by removing all HTML tags with library \texttt{BeaufitulSoup} \cite{beautifulsoup}.

For some documents we were not able to retrieve their contents using the abovementioned URLs. Most such documents did not have a version in English, or instead had a PDF version and no  HTML version. We decided not to handle those files because the vast majority of the \SI{138911}{} files from previous steps had an HTML version in English, and having to consider documents in other languages or parse PDF files would add another layer of complexity when we already had more than enough data for this project.

Each document's contents was stored in a separate \textit{.txt} file named \texttt{texts/<}CELEX identifier\texttt{>.txt}. A complete list of documents for which contents were retrieved is available at \texttt{texts.txt}, one CELEX identifier per line (this file is essentially equivalent to listing all files in directory \texttt{texts} without their extensions). Out of the \SI{138911}{} documents we knew of from previous data processing steps, we were able to find text for \SI{99903}{} ($71.92\%$). The file \texttt{texts.txt} has a size of \SI{1.2}{\mega\byte}, and the total size of the \texttt{texts/} folder containing all document texts we could obtain is \SI{1.55}{\giga\byte}.

\subsection{Data enrichment}

Finally, we processed the text files to remove newlines in cases where there were three or more sequential newlines, and combined the document metadata from \texttt{filtered.csv} with the document contents in \texttt{texts/}, keeping only the documents for which we were able to obtain contents. The result of this operation was stored in \texttt{combined.csv}, with \SI{99903}{} entries and a size of \SI{1.31}{\giga\byte}.

\section{Dataset fields}
\label{sec:dataset-fields}

After completing all steps of data preparation, we obtained a dataset \texttt{combined.csv} with \SI{99903}{} entries and a size of \SI{1.31}{\giga\byte}. It has the fields described in Table \ref{tab:fields}.
The class diagram for the dataset is presented in Figure \ref{fig:class-diagram}.
Dates are represented in the format \texttt{YYYY-MM-DD}, and each date field is zero-padded when needed.

\begin{table}[ht]
    \centering
    \caption{Fields of the final dataset.} \label{tab:fields}
    \begin{tabular}{@{}l|c|l@{}}
        \textbf{Field}               \textbf{Type}   & \textbf{Description}    \\ \hline
        \texttt{celex}                  & text            & CELEX identifier        \\
        \texttt{form}                   & category        & Form of document        \\
        \texttt{date}                   & date            & Date of signature       \\
        \texttt{title}                  & text            & Title                   \\
        \texttt{oj\_date}               & date            & Date of pub. in OJ      \\
        \texttt{of\_effect}             & date            & Date of effect          \\
        \texttt{end\_validity}          & date            & End of validity         \\
        \texttt{addressee}              & text            & Addressee(s)            \\
        \texttt{subject\_matter}        & list            & Subject matters         \\
        \texttt{directory\_codes}       & list            & Directory codes         \\
        \texttt{eurovoc\_descriptors}   & list            & EuroVoc descriptors     \\
        \texttt{legal\_basis}           & list            & IDs of legal basis      \\
        \texttt{relationships}          & list            & IDs of related docs.    \\
        \texttt{text}                   & text            & Content of document     \\
    \end{tabular}
\end{table}

\begin{figure}
  \includegraphics[width=\linewidth]{diagram-class-diagram.drawio}
  \caption{Class diagram for conceptual model.}
  \label{fig:class-diagram}
\end{figure}

\section{Data exploration}
\label{sec:data-exploration}

Data exploration was performed to assert information like common keywords, form types and distribution of document publication date. Data exploration was performed using a Jupyter Notebook, running chunks of Python3 code.

Missing data can be caused by a variety of factors, but it is mostly associated to human error and the evolving metadata needs, which means certain older documents have empty fields, probably because at the time the database did not have all the fields it currently does. Invalid data is originated mostly from platform limitations: if a platform requires a date to have three fields year/month/day but a document validity does not expire, users are forced to use some standardized solution for that case.

\begin{table}[ht]
    \centering
    \caption{Missing values per field.} \label{tab:missing}
    \begin{tabular}{@{}l|r|r@{}}
        \textbf{Field}              & \textbf{Missing}& \textbf{\%} \\ \hline
        \texttt{date}                   & 1 615            & 1.62                    \\
        \texttt{title}                  & 0               & 0.00                    \\
        \texttt{oj\_date}               & 1 971            & 1.97                    \\
        \texttt{of\_effect}             & 14 483           & 14.50                   \\
        \texttt{end\_validity}          & 17 398           & 17.41                   \\
        \texttt{addressee}              & 74 951           & 75.02                   \\
        \texttt{subject\_matter}        & 15 400           & 15.41                   \\
        \texttt{directory\_codes}       & 17 310           & 17.33                   \\
        \texttt{eurovoc\_descriptors}   & 20 386           & 20.41                   \\
        \texttt{legal\_basis}           & 6 567            & 6.57                    \\
        \texttt{relationships}          & 869             & 0.87                    \\
    \end{tabular}
\end{table}

\texttt{addressee} was found to be the column by far with the highest missing data percentage at 75.02\%, as per Table \ref{tab:missing}. This lack of data might be due to the fact that most documents are general European law, and as such are directed at all member states that are part of the EU, regardless of EU membership at the time of publishing.

Column \texttt{end\_validity} has 17.41\% missing elements, which may be due to the indefinite validity of a document. The list columns \texttt{subject\_matter}, \texttt{directory\_codes} and \texttt{eurovoc\_descriptors} range from 15.41\% to 20.41\% missing elements which may be due to the mere absence of list items (e.g., there are no EuroVoc terms associated to the document due to its nature), or due to human error when inputting records in the database, information loss or non-applicability of some fields in certain documents.\par

An interesting observation is that \texttt{relationships} is not present only in 0.87\% of the entries, revealing the strong connection between the vast majority of EU legislation: laws that replace, complement or build upon older ones.

\subsection{Dates}

Document dates range from 24 Sep 1949 to 4 Oct 2013. \texttt{oj\_date} and \texttt{of\_effect} distributions are very similar to \texttt{date}, increasing until 1981 (second EU enlargement). In the 1981-2001 period the number of documents per year grew slowly, until late 2000 when the Nice European Council decided to speed up accession negotiations with the 12 countries that would later join the EU in 2004, causing the number of documents signed in 2001 to increase by almost $60\%$ when compared with 2000. 
This number has stayed somewhat stable and even decreased since 2002. \texttt{end\_validity} values range from 7 Jan 1954 to 31 Dec 2058, with the exception of \SI{34833}{} documents that have as expiration date 1 Jan 2100, which we interpret as the document not expiring.

\begin{figure}[H]
  \includesvg[width=\linewidth]{date-histogram.svg}
  \caption{Date histogram.}
\end{figure}

\subsection{Lists}

The dataset has 249 unique subject matters, being ``common commercial policy'' the one with the highest frequency appearing \SI{11961}{} times, followed by ``External relations'' (\SI{10478}{}) and ``Agriculture'' (8 943).
The dataset uses \SI{5204}{} unique EuroVoc descriptors, with the most frequent being ``import''  (\SI{5871}{}), ``export refund'' (\SI{4785}{}) and ``originating product'' (\SI{3837}{}).
Each document is related to an average of 5.93 other documents. The documents most referred to are \texttt{32007R1234} (regulation on the common organisation of agricultural markets), \texttt{21994A0103(74)} (\textit{Agreement on the European Economic Area}) and \texttt{11957E113} (\textit{Treaty establishing the European Economic Community}).

\subsection{Text fields}

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{title-length-histogram.svg}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[width=\linewidth]{text-length-histogram.svg}
    \end{minipage}
    \caption{Text fields length histograms.}
\end{figure}

Title length varies between 6 and \SI{1531}{} characters, with an average length of 216.3 characters. The title length approximately follows a log-normal distribution, since it is mostly right-skewed and its tail (to the right) tends to get smaller quite slowly.
Text length is between 300 and 6.3M characters, with an average length of \SI{12583.3}{} characters. It presents a distribution similar to \texttt{title}.

\pagebreak

\section{Retrieval tasks}
\label{sec:retrieval-tasks}

Here we present some of the information needs we expect to meet:

\begin{enumerate}
    \item Amends to Decision 1999/468/EC
    \item Legislation related to Portugal
    \item Most recent regulation on Agriculture
    \item Competition-related documents in force
    \item Data protection regulation after 2010
    \item Legislation on import-export with Canada
    \item Fishing quotas in the North Atlantic Sea
\end{enumerate}

\section{Indexing}
We decided to use as information retrieval tool Solr 8.10 \cite{solrDocs}, given it is free and open software, and its more free text search-oriented approach when compared to other information retrieval software. Its workflow consists of importing data, indexing using a provided schema, specifying indexing and querying rules for each field, and finally using its query system to search and extract documents.

\subsection{Data importing} \label{ssec:data-importing}
From the previous stages of development, data was stored in CSV format, which made the importing step straightforward as the Solr endpoint accepts CSV for inserting/updating collections. We only needed to specify the field encapsulator \textit{"}, since fields \textit{title} and \textit{text} included commas, and had to be encapsulated to escape these delimiter characters.
\begin{verbatim} 
    curl -X POST 'http://localhost:8983/solr/docs/update
    /csv?commit=true&encapsulator="' \
    -T /data/combined.csv \
    -H 'Content-type:application/csv'
\end{verbatim}

There was initial concern with the size of the data file and possible issues with using it in the indexing tool. We initially filtered the data in the filtering stage \ref{ssec:filtering} to get a file of \textasciitilde\SI{1.8}{\mega\byte}, to assess if Solr was correctly configured and our dataset was being correctly imported and indexed. At a later stage, we switched to the full dataset, which proved to be less troublesome than expected as importing took only \SIrange{100}{200}{\second}.

\subsection{Indexing schema}
Solr indexing configurations are specified in a schema file, which assigns for each field a custom type and a set of operations to be performed on indexing and querying. The schema is uploaded through an endpoint similar to how the data was imported:
\begin{verbatim}
    curl -X POST -H 'Content-type:application/json' \
    --data-binary @/data/schema.json \
    http://localhost:8983/solr/docs/schema
\end{verbatim}

An example of a field type in our schema is as follows:

\begin{lstlisting}
{
  "name":"docTitle",
  "class":"solr.TextField",
  "indexAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ClassicFilterFactory"},
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  },
  "queryAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ClassicFilterFactory"},
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  }
}
\end{lstlisting}

% This field type is used to index the titles of each document, and the steps it takes are:
% \begin{enumerate}
%     \item \textit{"class"}: the Solr class used to store the index. Ranges from boolean to double point and even date types.
%     \item \textit{"indexAnalyser"}: Used upon ingestion of data to be indexed. Specifies the way indexes will be built and stored. 
%     \item \textit{"queryAnalyser"}: Used in execution of queries and aplied to values being searched for, which are then matched with the stored index's values.
% \end{enumerate}
% Both of these analysers are composed of a tokenizer and a series of filters. The tokenizers are in charge of breaking up the value into tokens, be it by using whitespace and punctuation as delimiters, as is the case of the \textit{StandardTokenizerFactory}, or by a specific set of delimiters, as is the case of the \textit{SimplePatternTokenizerFactory}, that uses regular expressions to match sequences (in our case, in the \textit{docList} type, to split the list of values by semicolons).

Our experiments showed that the default tokenizer and filter configurations are enough for most fields and cases, although some adjustments to the indexing and querying stages proved fruitful. 

A notable example is the \texttt{docText} field type, used to index each document's actual content.
Since this field has newline characters, which are of no value in searches, we use the \textit{PatternReplaceFilterFactory} filter to remove these from the indexing process, as well as a \textit{StopFilterFactory} filter to specify common English words to be ignored (mostly articles and prepositions). The list given in the default \texttt{stopwords.txt} file sufficed for our use case.

\begin{lstlisting}
{
  "name":"docText",
  "class":"solr.TextField",
  "large": true,
  "stored": true,
  "multiValued": false,
  "indexAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.PatternReplaceFilterFactory",
        "pattern":"\n", "replacement":" "},
      {"class":"solr.StopFilterFactory",
        "words":"stopwords.txt", "ignoreCase":true},
      {"class":"solr.KStemFilterFactory"}
    ]
  },
  "queryAnalyzer":{
    "tokenizer":{
      "class":"solr.StandardTokenizerFactory"
    },
    "filters":[
      {"class":"solr.ASCIIFoldingFilterFactory",
        "preserveOriginal":true},
      {"class":"solr.LowerCaseFilterFactory"},
      {"class":"solr.KStemFilterFactory"}
    ]
  }
}
\end{lstlisting}

\section{Queries}

The primary objective of using Solr's query system is to meet the information needs samples presented in section \ref{sec:retrieval-tasks}. We have translated those information needs into the following queries:

\begin{enumerate}
    \item Amends to Decision 1999/468/EC \begin{verbatim}
title:"1999/468/EC amend"~10^4
text:"1999/468/EC amend"~10
\end{verbatim}
    \item Legislation related to Portugal \begin{verbatim}
text:portugal
title:portugal^4
eurovoc_descriptors:portugal^8
\end{verbatim}
    \item Most recent regulation on Agriculture \begin{verbatim}
text:agriculture
title:agriculture^4
eurovoc_descriptors:agriculture^8
subject_matter:agriculture^8
\end{verbatim}
    \item Competition-related documents in force \begin{verbatim}
text:competition
title:competition^4
eurovoc_descriptors:competition^8
subject_matter:competition^8
\end{verbatim} Filters: \begin{verbatim}
of_effect:[* TO NOW]
end_validity:[NOW TO *]
\end{verbatim}
    \item Data protection regulation after 2010 \begin{verbatim}
title:"data protection"^4~2
eurovoc_descriptors:"data protection"^4~2
text:"data protection"~2
\end{verbatim} Filters: \begin{verbatim}
form:"regulation"
date:["2010-01-01T00:00:00Z" TO *]
\end{verbatim}
    \item Documentation on import-export with Canada \begin{verbatim}
title:"import Canada"^2~5
title:"export Canada"^2~5
text:"import Canada"~10
text:"export Canada"~10
eurovoc_descriptors:"import"^4
eurovoc_descriptors:"export"^4
eurovoc_descriptors:"Canada"^4
\end{verbatim}
    \item Fishing quotas in the North Atlantic Sea \begin{verbatim}
title:"fishing"^2
title:"quotas"^2
title:"North Atlantic"^4~1
text:"fishing"^2
text:"quotas"^2
text:"North Atlantic"^4~1
eurovoc_descriptors:"fishing"^2
eurovoc_descriptors:"quota"^2
eurovoc_descriptors:"North Sea"^4~1
\end{verbatim}
\end{enumerate}

We have used the Lucene query parser with query operator set to \texttt{OR} for all queries. Notice that we used filters for some queries.

\section{Result evaluation}

\subsection{Relevance analysis} \label{sec:relevance-analysis}

Evaluating recall performance measures requires one to know the complete set of relevant search results among all documents for each query. In our case, it is practically impossible to obtain the relevant sets because our dataset has a significant size (\SI{99903}{} documents), meaning we would have to judge each of those documents' relevance for each query.

To circumvent this issue, we estimated the relevant sets by assuming that the information retrieval system has a certain minimum quality, and that all documents that are relevant for a certain query will appear in the first $N = 100$ search results for that query. We will from that point on assume those \SI{100}{} documents constitute the whole dataset for that query, and manually evaluate the relevance of each of those documents for said query. This makes the manual relevance judgement more manageable as we have defined 7 sample retrieval tasks, and as such we only have to judge the relevance of at most 700 documents instead of the whole \SI{99903}{} documents.

We have decided that a user of our system is at most interested in the $n=10$ most relevant documents, as it is the typical number of results in a results page of webpage search engines, as well as the maximum amount of results we believe a user expects when performing an informational query.

To expedite the manual relevance judgement and calculation of performance measures, a small tool was developed. This is a monolithic HTML document with included CSS and Javascript code, which allows anyone to:
\begin{itemize}
    \item Import the Solr query results in JSON format
    \item Display those results in a user-friendly way for manual inspection and relevance judgement
    \item Easily select relevant documents
    \item Export the list of relevant documents
    \item See the most common performance measures, which are automatically calculated and updated based on user input
\end{itemize}

\subsection{Performance measures}

\begin{figure}[t]
    \centering
    \includesvg[width=\linewidth]{recall-precision.svg}
    \includesvg[width=\linewidth]{recall-precision-interpolated.svg}
    \caption{Recall-precision graphs.} \label{fig:recall-precision-graphs}
\end{figure}

We used the abovementioned HTML document to automatically get the performance measures after manual relevance judgement. We calculated the following measures for all queries:

\begin{itemize}
    \item Precision at $n$ (P@n): fraction of the $n$ first search results that are relevant.
    \item Recall at $n$ (R@n): fraction of relevant documents in the dataset which are in the $n$ first search results.
    \item F-measure at $n$ (F@n): a score that balances recall and precision for a query showing $n$ results using a formula similar to a geometric mean. We are using $\beta=1$ until further thought is put into which criteria is more important in our use case, if any.
    \item Average Precision (AvP): arithmetic average of the values of P@n for values $n$ where relevant documents occur, for $1 \leq n \leq N$, where $N$ is the dataset size (which, due to the approximations mentioned in section \ref{sec:relevance-analysis}, is at most $N=100$).
    \item Mean Average Precision (MAP): arithmetic mean of the AvP for a set of queries using the same information retrieval system, used as a global system performance score.
\end{itemize}

We calculated P@n, R@n and F@n for all 7 queries and all valid values of $n$ for each query (e.g., some queries returned less than $N=100$ results) and AvP for all queries, presented in Table \ref{tab:ir-quality}. Since $n=10$ was considered the most relevant number of search results, we only show P@10, R@10 and F@10. Further system performance information is shown in Figure \ref{fig:recall-precision-graphs}.

We also calculated the value of MAP, which is 0.68 for the system at its current iteration.

\begin{table}[ht]
    \centering
    \caption{Information retrieval effectiveness measures.} \label{tab:ir-quality}
    \begin{tabular}{@{}l|r r r r@{}}
        \textbf{Query}   & AvP  & P@10 & R@10 & F@10 \\ \hline
        Query 1          & 0.50 & 0.40 & 0.57 & 0.47 \\
        Query 2          & 0.85 & 0.80 & 0.10 & 0.17 \\
        Query 3          & 0.30 & 0.40 & 0.13 & 0.20 \\
        Query 4          & 1.00 & 1.00 & 0.10 & 0.18 \\
        Query 5          & 0.55 & 0.50 & 0.50 & 0.50 \\
        Query 6          & 0.93 & 1.00 & 0.14 & 0.25 \\
        Query 7          & 0.60 & 0.70 & 0.12 & 0.21 \\
    \end{tabular}
\end{table}

These results reveal that the search system is working decently, with some queries giving almost-perfect results (Q2, Q4, Q5), awful (Q3) and reasonable (Q1, Q6, Q7). The problems revealed by the queries with the worst scores were considered and reflected on possible future work.

\section{Future work}

We intend to use other query parsers, namely \textit{DisMax} and \textit{eDisMax}, which will be used to assess possible improvements in query results quality.
We also intend to implement a simple front-end interface for the search system, although it is not a priority compared to the following improvements.

\subsection{Related documents}
On all queries, the ability to show the top related documents, which would be implemented in the following way: Execute the input query, and then do another query but the 10-best ranked CELEX from the first are collected, and documents `related' to those 10 best-ranked CELEX (as in, present in their \texttt{relationships} field) are weighted more on the 2nd query; the final results are those of the 2nd query. 
    
In practice, this would show the top 10 documents from the input query along with all the documents reference by those. This could be visually improved using grouping, though that is still an effort to be explored.

\subsection{Faceting}
Making use of faceting on fields with tag-like structure such as \texttt{directory\_codes}, \texttt{eurovoc\_descriptors} and \texttt{addressee} to arrange search results into groups would complement the use of queries, since it would show the specific tags already in the documents, without the need for the user to guess the correct terminology.

\subsection{Synonyms in queries}
Another feature that could be of aid is the use of the \textit{SynonymGraphFilter} in the query phase, to spread out the terms that encompass the query. This, however, has the limitation of the size of the synonym list to input into the system, since our database encompasses a wide variety of topics which would require an extensive list of synonyms, which may not scale well for our application. We will pursue the faceting option first, and test the synonyms approach if performance is not significantly affected and if we have time.

\subsection{Date as scoring criteria}
Say we want the regular scoring to be performed based on token occurrence, but we would prefer that more recent documents come up first. We don't want to sort results by date, because if we're searching for a very common keyword we'll get many documents that are from the last year we collected data for; instead, we would like to use both date and score for ranking, so a reasonably old but very well-scored document comes up at the top, and very recent documents without a decent score come up towards the end.

\subsection{Page rating}
Implement a document-rating algorithm. If we're searching about a generic topic of interest, there will be many document matches, many of which will be about a specific topic somehow related to our topic of interest, which got well scored just because they incidentally mention our topic of interest very often. We would rather prefer to better rank documents based on the following characteristics, which can be fully or partially computed \textit{a priori}.

Consider the following documents to be used in the examples below:
\begin{itemize}
    \item \textbf{D1}: \texttt{32007R1234} -- \textit{Council Regulation (EC) No 1234/2007 of 22 October 2007 establishing a common organisation of agricultural markets and on specific provisions for certain agricultural products}
    \item \textbf{D2}: \texttt{32013R0849} -- \textit{Commission Implementing Regulation (EU) No 849/2013 of 2 September 2013 establishing the standard import values for determining the entry price of certain fruit and vegetables} (consider as a simplification that this document is about the transport conditions of oranges in the EU)
\end{itemize}

We will assume we are doing a search for ``agriculture'', and that both documents mention ``agriculture'' about as often.

\subsubsection{Specificity}

We find D1 more relevant for this search, because ``agriculture'' is one of the few keywords of D1, while D2 mentions ``agriculture'' as often but also mentions some uncommon topics like ``produce transport'', ``citrus'' and ``oranges'', in increasing order of specificity.

When asked to answer the question \textit{What are these two documents about?}, a bad answer is that both are about agriculture (based on mere occurrence counting), and a good answer is that D1 is about ``agriculture'' and D2 is about ``oranges'', because even if D2 may not refer to ``oranges'' as much as it does to ``agriculture'', ``oranges'' is a much rarer and more specific topic than ``agriculture'', and must be weighed in as such.

Thus, if we're searching for ``agriculture'', we want documents that mention ``agriculture'' along many other topics and more specific topics to be lower in the ranking, and documents that almost only mention ``agriculture'' (even if not that often) should be ranked higher in the ranking.

\subsubsection{Importance}

If two documents are tied using some criteria, we would like to get first the document that is more \textit{important}. This is obviously an ambiguous concept, but for this effect we can consider a document \textit{more important} than another if, when those two documents are presented side-by-side, someone would rather open one document first than the other, because that person believes one of them is considered to have greater authority on that issue, or because that document is referenced more often in other documents.

If we choose to stick with the latter definition of importance measured by references, we are left with a classical page-rating problem where we only know the structure of the page graph. We can employ such a scoring system because attributes \texttt{legal\_basis} and \texttt{relationships} provide us with the connections between documents.

As a future work, we can implement the PageRank algorithm to evaluate documents' importance, and weight in that factor in ranking order.

\section{Conclusion}

The fundamental goals of this project were to search and select relevant datasets, perform data exploration analysis, assess the datasets' quality, characterize its fields and identify information needs for the system to be developed. We were further expected to index our dataset using an information retrieval tool and perform basic querying tasks, as well as evaluate the returned results' relevance.
% On the third and final iteration, we are expected to improve upon this system and finalize the product by making use of features and techniques with the goal of improving the quality of the search results.

We obtained a large dataset with data as coherent and complete as possible for the subset of documents we chose to analyse. We indexed our dataset using Solr, tested and evaluated the results for a few queries and manually evaluated the results for these in terms of relevance.

\bibliographystyle{ACM-Reference-Format}
\bibliography{\mainabsdir/../bib/bibliography-file.bib}

\end{document}
\endinput
